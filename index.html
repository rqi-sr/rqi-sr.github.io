<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Rethinking Image Evaluation in Super-Resolution">
  <meta property="og:title" content="Rethinking Image Evaluation in Super-Resolution"/>
  <meta property="og:description" content="Rethinking Image Evaluation in Super-Resolution"/>
  <meta property="og:url" content="https://rqi-sr.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="RQI Super Resolution Quality Assessment">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Rethinking Image Evaluation in Super-Resolution</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Rethinking Image Evaluation in</br>Super-Resolution</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=hvzOCpAAAAAJ&hl=ca&oi=ao" target="_blank">Shaolin Su</a><sup>1</sup>,</span>
                <span class="author-block">Josep M. Rocafort</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://dxue321.github.io/" target="_blank">Danna Xue</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://davidserra9.github.io/" target="_blank">David Serrano-Lozano</a><sup>1,2</sup>,</span>
                      <span class="author-block">
                        <a href="https://ahupujr.github.io/" target="_blank">Lei Sun</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a href="https://jvazquezcorral.github.io/" target="_blank">Javier Vazquez-Corral</a><sup>1,2</sup>
                  </span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Computer Vision Center, <sup>2</sup>Universitat Autònoma de Barcelona, <sup>3</sup>
INSAIT, Sofia University</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2503.13074" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>ArXiv</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/SSL92/RQI" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code - Coming Soon</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/Fig1-1.png" alt="Descriptive caption" style="width: 120%; height: auto;">
      <figcaption class="is-size-5.5 has-text-left mt-2">
        We show that even Ground Truth (GT) images (middle) in existing SR datasets can show relatively poor quality. As a result, image metrics tend to favor outputs that more resemble the reference GTs, even when they are perceptually poorer (left side), leading to contradictory evaluations with human preferences (right side). Here, we analyze how GT quality affects the evaluation of SR models and propose RQI to fairly assess SR models with imperfect GTs.
      </figcaption>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While recent advancing image super-resolution (SR) techniques are continually improving the perceptual quality of their outputs, they can usually fail in quantitative evaluations. This inconsistency leads to a growing distrust in existing image metrics for SR evaluations. Though image evaluation depends on both the metric and the reference ground truth (GT),  researchers typically do not inspect the role of GTs, as they are generally accepted as `perfect' references. However, due to the data being collected in the early years and the ignorance of controlling other types of distortions, we point out that GTs in existing SR datasets can exhibit relatively poor quality, which leads to biased evaluations. Following this observation, in this paper, we are interested in the following questions: Are GT images in existing SR datasets 100% trustworthy for model evaluations? How does GT quality affect this evaluation? And how to make fair evaluations if there exist imperfect GTs? To answer these questions, this paper presents two main contributions. First, by systematically analyzing seven state-of-the-art SR models across three real-world SR datasets, we show that SR performances can be consistently affected across models by low-quality GTs, and models can perform quite differently when GT quality is controlled. Second, we propose a novel perceptual quality metric, Relative Quality Index (RQI),  that measures the relative quality discrepancy of image pairs, thus issuing the biased evaluations caused by unreliable GTs. Our proposed model achieves significantly better consistency with human opinions. We expect our work to provide insights for the SR community on how future datasets, models, and metrics should be developed.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="hero">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-three-fifths has-text-centered">
        <h2 class="title is-3 has-text-left">How GT quality affects SR model evaluations?</h2>
        <figure>
          <img src="static/images/Fig2.png" alt="Descriptive caption">
          <figcaption class="is-size-5.5 has-text-left mt-2">
            By gradually discarding low quality GTs from testing datasets (images are from DIV2K, RealSR and DRealSR) and evaluate on the remaining high quality GTs, we make several observations: 1. Challenging images will always be challenging (for all the models, similar performance fluctuations occur when the same image is discarded). 2. High quality GTs are more challenging for SR models (by observing a consistent performance drop for all the models and on all the metrics). 3. Evaluation results can be different when GT quality is controlled (for example, SeeSR moves from  Rank #6 to Rank #2 by LPIPS and from Rank #6 to Rank #1 by DISTS, by considering only high quality GTs). 4. The perception-distortion tradeoff also exists.
      </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-three-fifths has-text-centered">
        <h2 class="title is-3 has-text-left">How to make fair evaluations with imperfect GTs?</h2>
        <figure>
          <img src="static/images/Fig3.png" alt="Descriptive caption" style="width: 500px; height: auto;">
          <figcaption class="is-size-5.5 has-text-left mt-2">
            Our solution is straightforward and simple: Since we do not recognize GTs as perfect references, we allow cases in which model outputs can achieve better quality than the GT. Therefore, we propose RQI (Relative Quality Index) to measure the relative quality from target images to GTs. RQI differs from traditional FR-IQA scheme in three aspects: 1. RQI is dependent with the input order. 2. We substitute reference image $I_0$ to any image $I_i$ in the distorted image sequence to cover complicated cases where GTs contain varying distortions. 3. We calculate relative quality discrepancy as label.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-left">Quantitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item is-flex is-justify-content-center">
        <!-- Your image here -->
        <img src="static/images/Fig4.png" alt="MY ALT TEXT" style="width: 500px; height: auto;"/>
      </div>
      <div class="item is-flex is-justify-content-center">
        <!-- Your image here -->
        <img src="static/images/Fig5.png" alt="MY ALT TEXT" style="width: 1000px; height: auto;"/>
      </div>
      <div class="item is-flex is-justify-content-center">
        <!-- Your image here -->
        <img src="static/images/Fig6.png" alt="MY ALT TEXT" style="width: 1000px; height: auto;"/>
     </div>
    </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-left">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/Fig8-1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle  is-size-6 has-text-left mt-2">
          We show different cases where existing metrics can fail. Up-left: distortion-based FR-IQA metric SSIM tend to favor blurry regions rather than perceptually good textures. Up-right: NR-IQA metric ClipIQA cannot handle cases where image semantics are changed due to the lack of proper reference. Bottom: perception-based FR-IQA metrics LPIPS and DISTS can fail when model outputs are perceptually better than GTs. As a comparison, RQI handles all the cases correctly. All scores are normalized to [0,1] for easier comparisons.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Fig10-1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle  is-size-6 has-text-left mt-2">
          More examples of distortion-based FR-IQA metrics SSIM and PSNR, which tend to favor blurry regions over textures, leading to contradictory predictions with human perception.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Fig11-1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle  is-size-6 has-text-left mt-2">
          More examples of NR-IQA metrics PI, NIQE, Clip-IQA and MANIQA, which fail on cases where subtle structure of semantics are changed, due to the lack of proper references.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/Fig12-1.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle  is-size-6 has-text-left mt-2">
        More examples of perception-based FR-IQA metrics LPIPS and DISTS, which can fail when GT quality is relatively lower. They make contradictory evaluations for models that output perceptually higher results than GTs.
     </h2>
   </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{su2025rethinking,
        title={Rethinking Image Evaluation in Super-Resolution},
        author={Su, Shaolin and Rocafort, Josep M and Xue, Danna and Serrano-Lozano, David and Sun, Lei and Vazquez-Corral, Javier},
        journal={arXiv preprint arXiv:2503.13074},
        year={2025}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
  <div class="container is-max-desktop">
    <h2 class="title">Acknowledgements</h2>
    <p class="content">
      This work was supported by the HORIZON MSCA Project funded by the European Union (project number 101152858), Grant PID2021-128178OB-I00 funded by MCIN/AEI/10.13039/501100011033, ERDF ``A way of making Europe'', the Departament de Recerca i Universitats from Generalitat de Catalunya with ref. 2021SGR01499. Shaolin Su was supported by the HORIZON MSCA Postdoctoral Fellowships. Danna Xue was supported by the grant Càtedra ENIA UAB-Cruïlla (TSI-100929-2023-2) from the Ministry of Economic Affairs and Digital Transition of Spain. David Serrano-Lozano was supported by the FPI grant from Spanish Ministry of Science and Innovation (PRE2022-101525). Lei Sun was partially funded by the Ministry of Education and Science of Bulgaria’s support for INSAIT as part of the Bulgarian National Roadmap for Research Infrastructure.

    </p>
  </div>
</div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
